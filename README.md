# Vynapse

> A Rust-native deep learning and neuroevolution engine built as a hybrid learning runtime ‚Äî unifying the optimization paradigms of PyTorch, TensorFlow, DEAP, and NEAT into one modular, high-performance system.

---

## üöÄ Project Description

Vynapse is a multi-paradigm machine learning engine written entirely in safe Rust. It bridges **gradient-based learning** and **evolutionary optimization** within a unified execution and graph infrastructure. Inspired by the flexibility of PyTorch, the structural clarity of TensorFlow, and the adaptive search power of DEAP and NEAT, Vynapse supports:

- **SGD-style training with reverse-mode autodiff**
- **Topology-evolving neural networks using NEAT**
- **Population-based weight evolution in DEAP style**
- **Static graph execution with forward/backward scheduling**

Every training mode operates on the same core tensor and graph runtime, enabling flexible, hybrid workflows that can evolve, fine-tune, and deploy networks interchangeably.

---

## üß† What Makes Vynapse Different?

- **Hybrid Optimization**: Combine evolution and gradient descent in a single training loop (Lamarckian or Baldwinian modes).
- **Neuroevolution-Native**: Evolve both weights and graph structures with speciation, compatibility distance, and reproduction strategies.
- **Autodiff Engine**: Full reverse-mode autodiff with support for eager and static graph execution.
- **Rust Performance**: Built from the ground up in safe, parallel Rust ‚Äî no Python bindings, no unsafe blocks.
- **Runtime Unification**: A single graph engine powers static, dynamic, differentiable, and evolutionary workflows.

---

## üéØ Immediate Plans

### Core Architecture Refactoring (Modular Task/Loss/Activation System)

- [x] **Create Task Trait** - Define a generic `Task` trait that specifies dataset generation, input/output dimensions, and task identification for all learning problems
- [x] **Move XOR to Task Implementation** - Refactor existing XOR logic from fitness function into a dedicated `XorTask` struct implementing the Task trait
- [x] **Create Powers of Two Task** - Implement `PowersOfTwoTask` for the MVP learning problem (teaching genomes to predict powers of 2: 1‚Üí2, 2‚Üí4, 3‚Üí8, etc.)
- [x] **Create Loss Trait** - Define a generic `Loss` trait for pluggable loss function implementations with standardized calculate and naming methods
- [x] **Implement MSE Loss Function** - Create `MeanSquaredError` struct implementing the Loss trait for regression-style fitness evaluation
- [x] **Create Activation Trait** - Define a generic `Activation` trait for pluggable activation functions with single-value and tensor-wide application methods
- [x] **Implement Sigmoid Activation** - Create dedicated `Sigmoid` struct implementing the Activation trait with proper mathematical implementation
- [ ] **Remove Sigmoid from Tensor Module** - Clean up tensor module by removing activation-specific logic, maintaining separation of concerns
- [ ] **Create Generalized Fitness Function** - Build `TaskBasedFitness<T, L, A>` struct that combines any Task, Loss, and Activation via generics
- [ ] **Implement Fitness Evaluation Logic** - Complete the modular fitness evaluation pipeline that runs datasets through networks with configurable components

> **Goal**: Transform the current tightly-coupled XOR fitness function into a flexible, "plug-and-play" system where tasks, loss functions, and activations can be mixed and matched without code changes.

---

## üó∫Ô∏è Roadmap Overview

- [x] Milestone 1: Weight Evolution (DEAP-style) *(currently in progress)*
- [ ] Milestone 2: Multi-Mode Trainers (DEAP, NEAT, SGD, Static)
- [ ] Milestone 3: Autodiff & Speciation
- [ ] Milestone 4: Hybrid Learning + Dataset API
- [ ] Milestone 5: GPU, WASM, Distributed Runtime
- [ ] Milestone 6: Research & Exploration

---

## üìç Milestone 1 ‚Äì *Genesis* (Weight-Evolution MVP)  
> Prove the core concept by evolving static network weights on XOR.

### üõ†Ô∏è Systems Foundations
- [x] Establish a multi-crate workspace (`core`, `cli`, `math`)
- [x] Design a project-wide error hierarchy with recoverable and fatal classes
- [ ] Integrate a fast, seedable PRNG and structured log sink

### üìê Numeric & Memory Primitives
- [ ] Draft a contiguous, heap-allocated tensor abstraction (row-major, shape + stride metadata)
- [ ] Implement element-wise operations and small fixed-size matrix multiplication using cache-aware loop order
- [ ] Add invariant checks for dimension mismatches and overflow guarding

### üß¨ Evolutionary Pipeline (DEAP-Style)
- [ ] Specify genome layout for a fixed 2-2-1 MLP (flat f32 buffer)
- [ ] Create Gaussian mutation with adaptive œÉ decay and optional per-weight masking
- [ ] Implement (Œº + Œª) and (Œº, Œª) selection strategies with tournament and roulette variants
- [ ] Build a generation scheduler with early-exit on convergence

### üîÅ Experiment Runner
- [ ] Provide CLI workflow to scaffold XOR configuration, inject hyper-parameters, and stream fitness stats
- [ ] Persist per-generation CSV logs and a compressed binary checkpoint of the best genome

### ‚úîÔ∏è Acceptance Criteria
- [ ] Converge to XOR loss < 1e-2 in < 5 s on 8-thread desktop CPU
- [ ] Achieve deterministic results on fixed seed; variance report < ¬±5 %

---

## üìç Milestone 2 ‚Äì *Quad-Core Minimal* (DEAP, NEAT, SGD, Static-Graph)  
> Train the *same* XOR network with four distinct optimisation paradigms.

### üèóÔ∏è Graph & Tensor Layer
- [ ] Extend tensor core with broadcast semantics, strided slices, and BLAS-style GEMM micro-kernel (na√Øve C = Œ±AB + Œ≤C)
- [ ] Introduce a DAG representation for computation graphs (node op, edge tensor ref) with acyclic validation
- [ ] Implement DOT and JSON exporters for graph visualisation and diffing

### üîÑ Trainer Implementations
- [ ] Port DEAP logic to pluggable `Trainer` interface
- [ ] Implement minimal NEAT topology mutation and feed-forward executor
- [ ] Add manual back-prop pipeline with simple optimiser for SGD mode
- [ ] Create declarative static-graph builder and reversed gradient pass

### üîß Config & CLI
- [ ] Build YAML/TOML experiment loader with spawn-time validation
- [ ] Add unified CLI flag `--mode={deap, neat, sgd, static}` with hyper-parameter overrides

### üìä Benchmark Harness
- [ ] Collect wall-clock time, CPU utilisation, and memory footprint per mode
- [ ] Generate Markdown comparison table via post-run script

### ‚úîÔ∏è Acceptance Criteria
- [ ] All four modes hit loss < 2e-2 within configurable budget
- [ ] Runtime variance between repeated runs < 10 %

---

## üìç Milestone 3 ‚Äì *Autodiff & Dynamic Graphs*  
> Introduce full reverse-mode autodiff and robust NEAT speciation.

### ‚öôÔ∏è Reverse-Mode Engine
- [ ] Design tape data-structure with parent indices, saved intermediates, and gradient function pointer
- [ ] Implement zero-grad, gradient accumulation (Œ£ from multiple downstream nodes), and retention policy
- [ ] Provide optimisers: SGD, momentum, Adam, RMSprop, and gradient clipping (L2 norm ceiling)

### üß© Dynamic Eager Runtime
- [ ] Overload tensor operations to record tape entries transparently (eager mode)
- [ ] Add runtime shape inference, alignment checks, and broadcast expansion
- [ ] Provide `trace!` utility converting eager runs to static DAG for re-execution

### üß¨ NEAT 2.0
- [ ] Implement compatibility distance metric (c‚ÇÅ, c‚ÇÇ, c‚ÇÉ) with user-defined weights
- [ ] Add species management: bucket assignment, stagnation detection, elite carry-over, dynamic œÉ scaling
- [ ] Parallelise population evaluation with a scoped thread-pool and work-stealing

### üìà Task Benchmark Suite
- [ ] Integrate 2-Spiral classification and CartPole reinforcement learning tasks
- [ ] Auto-generate loss / reward plots and termination statistics

### ‚úîÔ∏è Acceptance Criteria
- [ ] Back-prop finite-difference error < 1e-4 relative tolerance across ops
- [ ] Reach ‚â• 80 % code-coverage target in CI with deterministic, seed-locked tests

---

## üìç Milestone 4 ‚Äì *Hybrid Optimiser & Dataset Fabric*  
> Merge evolutionary search with gradient descent; add real-world datasets and experiment automation.

### üîÄ Hybrid Learning Layer
- [ ] Implement Lamarckian / Baldwinian pipeline: evolve population ‚Üí fine-tune top-k ‚Üí fitness re-evaluation ‚Üí merge
- [ ] Support composite fitness weighting (accuracy, parameter count, FLOPs, latency)

### üóÑÔ∏è Dataset Management
- [ ] Create memory-mapped CSV ingestion and image-folder loader with optional caching
- [ ] Implement mini-batch shuffler, deterministic epoch splits, and lazy prefetch into thread-local buffers
- [ ] Add augmentation hooks (crop, flip, noise) for images

### üìä Observability & Telemetry
- [ ] Integrate hierarchical tracing spans with wall-time and CPU counters
- [ ] Expose Prometheus metrics endpoint guarded by CLI flag
- [ ] Provide live progress UI via terminal dashboard

### üñ•Ô∏è Experiment Workflow
- [ ] Build CLI helpers for dataset splitting, provenance hashing, and cached preprocessing
- [ ] Define YAML recipe schema for reproducible experiment sweeps

### ‚úîÔ∏è Benchmarks & Targets
- [ ] Achieve MNIST ‚â• 98 % test accuracy (SGD) and ‚â• 96 % via hybrid in ‚â§ 10 epochs
- [ ] Ensure Prometheus scrape latency < 10 ms and live UI refresh interval ‚â§ 0.5 s

---

## üìç Milestone 5 ‚Äì *GPU, WASM & Distributed Scale-Out*  
> Accelerate with GPUs, run in browsers, and scale neuroevolution across nodes.

### ‚ö° GPU Compute
- [ ] Add WGSL compute kernels for matmul, activation, and reduction ops
- [ ] Provide GPU memory pool with pinned host buffers and asynchronous copy queues
- [ ] Implement optional CUDA PTX backend with occupancy querying

### üåê WASM Runtime
- [ ] Port tensor and graph engine to `no_std` with custom allocator
- [ ] Deliver browser demo running neuroevolution of XOR inside a WebWorker
- [ ] Build WASI command-line binary for edge deployments

### üåç Distributed Evolution
- [ ] Define gRPC protocol for remote genome evaluation jobs
- [ ] Implement node heartbeat, cluster fault detection, and checkpoint-based resume
- [ ] Support island model with periodic genome migration and fitness sharing

### üõ†Ô∏è CI / CD & Packaging
- [ ] Configure GitHub Actions for Linux, macOS, Windows, wasm32, and CUDA targets
- [ ] Automate release bundling, version tagging, changelog generation, and crates.io publication
- [ ] Enforce dual MIT / Apache-2.0 licensing with automated header checks

### ‚úîÔ∏è Acceptance Criteria
- [ ] Achieve ‚â• 10√ó speed-up over CPU baseline on 512 √ó 512 matmul using WGSL
- [ ] Complete 1 000 XOR generations in < 5 s in browser demo
- [ ] Demonstrate near-linear scaling to 32 worker processes in distributed mode

---

## üìç Milestone 6 ‚Äì *Research & Stretch Ventures*  
> Exploratory paths after core roadmap completion.

| Track | Objectives |
|-------|------------|
| **Differentiable NEAT** | Back-prop through mutable topologies for joint weight+structure optimisation |
| **Reinforcement Learning** | Add Gymnasium bridge, policy gradients, neuro-evolutionary actor/critic hybrids |
| **Meta-Evolution** | Evolve optimiser hyper-parameters or hyper-networks that generate task-specific sub-nets |
| **Hardware Autotuning** | Perform Bayesian search for GPU tile sizes, vector widths, register blocking |
| **Mobile Inference** | Produce Flutter / iOS / Android demos via `wgpu-rs` and cross-compiled WASM |
| **Compiler Fusion** | Integrate Xyntra-generated fused kernels as node implementations for accelerated inference |
| **Sparse Execution** | Implement graph-level dynamic pruning, activation gating, and runtime re-wiring |

---
